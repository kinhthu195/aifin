{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://certificate.tpq.io/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workshop at Texas State University (October 2023)**\n",
    "\n",
    "**_Transformers & Attention_**\n",
    "\n",
    "Dr. Yves J. Hilpisch | The Python Quants GmbH | http://tpq.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Let's start with the basics:\n",
    "\n",
    "### Transformer\n",
    "\n",
    "Imagine you're reading a book, and while reading a sentence, you remember something related from a few pages back. This connection helps you better understand the current sentence. A transformer does something similar but for machines. It's a method that helps machines pay \"attention\" to different parts of data (like words in a sentence) to understand it better. It can look at all words at once and decide which ones are important in context.\n",
    "\n",
    "For example, in the sentence \"He dropped the glass because it was too hot,\" understanding \"hot\" helps us know why the glass was dropped.\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "GPT is like a super-smart student. Before giving it any specific task, it reads (or \"trains\" on) a huge amount of text from the internet. This is the \"pre-trained\" part. It learns language, facts, reasoning abilities, and even some mistakes from this data. Then, when you give it a specific question or task, it uses all that knowledge to generate (or \"write\") a coherent and contextually relevant response.\n",
    "\n",
    "### Other Key Topics:\n",
    "\n",
    "1. **Attention**: This is like the spotlight of our memory. When we think or read, we don't give equal importance to everything. Some parts get more focus because they're more relevant at the moment. In transformers, \"attention\" helps the model decide which parts of the data to focus on more.\n",
    "\n",
    "2. **Embeddings**: Think of this as translating words into a secret language that computers understand better. This language is in the form of numbers. So, \"cat\" might be translated to a list of numbers, which captures the essence or meaning of \"cat\" in this numeric form.\n",
    "\n",
    "3. **Fine-tuning**: Remember our super-smart student, GPT? After it learns from the internet, it can be given specific examples to make it even better at certain tasks. This process is like giving it extra classes on a particular subject.\n",
    "\n",
    "4. **Self-Attention**: This is a special feature of transformers. When trying to understand a word, the model looks at all other words in the sentence, not just neighboring words. It's like reading a whole paragraph to understand a single complex word in it.\n",
    "\n",
    "5. **Layers**: Transformers have multiple layers, just like how a complex thought might be built on simpler ideas. Each layer in a transformer captures different levels of understanding from the data.\n",
    "\n",
    "6. **Tokens**: These are chunks of text the model looks at. A token can be as short as one character or as long as one word.\n",
    "\n",
    "In essence, transformers, like GPT, are advanced tools that help machines understand and generate human-like text by focusing on the right parts of data, remembering what they've learned, and building complex understanding layer by layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are a fundamental concept in natural language processing (NLP) and deep learning. They provide a way to represent words (or other entities) as dense vectors of real numbers. The idea is to represent semantic meaning in a continuous vector space, where the distance and direction between vectors correspond to semantic differences between the words.\n",
    "\n",
    "### Importance of Embeddings:\n",
    "\n",
    "1. **Dimensionality Reduction**: Natural language has a vast vocabulary. One-hot encoding words would result in extremely high-dimensional vectors with a dimension equal to the vocabulary size. Embeddings reduce this to a manageable size (e.g., 50, 100, 300 dimensions).\n",
    "  \n",
    "2. **Semantic Relationships**: Good embeddings capture semantic relationships. For instance, the vector difference between \"king\" and \"man\" might be similar to the difference between \"queen\" and \"woman\".\n",
    "  \n",
    "3. **Transfer Learning**: Pre-trained embeddings can be used in new tasks, allowing models to benefit from knowledge captured in the embeddings without requiring extensive training data for the new task.\n",
    "\n",
    "### How are they derived?\n",
    "\n",
    "There are various methods to derive word embeddings:\n",
    "\n",
    "1. **Word2Vec**: Uses a shallow neural network to either predict a word given its context (Skip-Gram) or predict the context given a word (CBOW). The weights of this model then serve as the word embeddings.\n",
    "  \n",
    "2. **GloVe (Global Vectors for Word Representation)**: Constructs a co-occurrence matrix from a corpus and then factorizes this matrix to produce embeddings.\n",
    "  \n",
    "3. **FastText**: An extension of Word2Vec, it treats each word as composed of character n-grams. This allows it to generate embeddings for out-of-vocabulary words.\n",
    "  \n",
    "4. **Embeddings from Deep Learning Models**: Models like ELMo, BERT, and GPT generate contextual embeddings where the embedding for a word is based on its context in a sentence, and not just the word itself.\n",
    "\n",
    "### Python Example with Word2Vec:\n",
    "\n",
    "We'll use the `gensim` library to demonstrate Word2Vec. Let's create a simple model using some sentences.\n",
    "\n",
    "(Note: For this example, I'll provide code and expected outputs since we can't run the `gensim` library directly here.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07511582 -0.00930042  0.09538119 -0.07319167 -0.02333769 -0.01937741\n",
      "  0.08077437 -0.05930896  0.00045162 -0.04753734]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['hello', 'world'],\n",
    "    ['hello', 'everyone'],\n",
    "    ['goodbye', 'world'],\n",
    "    ['goodbye', 'everyone']\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Accessing the word vector for 'hello'\n",
    "vector_hello = model.wv['hello']\n",
    "\n",
    "print(vector_hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, you can also find similar words, perform vector arithmetic to discover semantic relationships, and more.\n",
    "\n",
    "Remember, real-world embeddings are usually trained on vast corpora with billions of words, and as such, the embeddings capture richer semantic meanings than the small example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Advanced Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step guide to generating word embeddings using Word2Vec and a simple use case of leveraging those embeddings.\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "We'll begin byimporting the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the Data\n",
    "\n",
    "For this example, let's use some sample sentences. In a real-world scenario, you'd likely have a much larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['dog', 'barks'],\n",
    "    ['cat', 'meows'],\n",
    "    ['bird', 'sings'],\n",
    "    ['fish', 'swims'],\n",
    "    ['horse', 'gallops'],\n",
    "    ['koala', 'climbs'],\n",
    "    ['dolphin', 'jumps']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842, 14000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the Embeddings\n",
    "\n",
    "To visualize the embeddings in 2D space, we can use PCA (Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"504.498153pt\" height=\"335.465312pt\" viewBox=\"0 0 504.498153 335.465312\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-25T04:16:25.685873</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.8.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 335.465312 \n",
       "L 504.498153 335.465312 \n",
       "L 504.498153 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 39.501563 312.12 \n",
       "L 485.901562 312.12 \n",
       "L 485.901562 7.2 \n",
       "L 39.501563 7.2 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 58.017533 312.12 \n",
       "L 58.017533 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\"/>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −0.03 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(45.366752 326.277812) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-2212\" d=\"M 3381 1997 \n",
       "L 356 1997 \n",
       "L 356 2522 \n",
       "L 3381 2522 \n",
       "L 3381 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-30\" d=\"M 266 2259 \n",
       "Q 266 3072 433 3567 \n",
       "Q 600 4063 929 4331 \n",
       "Q 1259 4600 1759 4600 \n",
       "Q 2128 4600 2406 4451 \n",
       "Q 2684 4303 2865 4023 \n",
       "Q 3047 3744 3150 3342 \n",
       "Q 3253 2941 3253 2259 \n",
       "Q 3253 1453 3087 958 \n",
       "Q 2922 463 2592 192 \n",
       "Q 2263 -78 1759 -78 \n",
       "Q 1097 -78 719 397 \n",
       "Q 266 969 266 2259 \n",
       "z\n",
       "M 844 2259 \n",
       "Q 844 1131 1108 757 \n",
       "Q 1372 384 1759 384 \n",
       "Q 2147 384 2411 759 \n",
       "Q 2675 1134 2675 2259 \n",
       "Q 2675 3391 2411 3762 \n",
       "Q 2147 4134 1753 4134 \n",
       "Q 1366 4134 1134 3806 \n",
       "Q 844 3388 844 2259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-2e\" d=\"M 581 0 \n",
       "L 581 641 \n",
       "L 1222 641 \n",
       "L 1222 0 \n",
       "L 581 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-33\" d=\"M 269 1209 \n",
       "L 831 1284 \n",
       "Q 928 806 1161 595 \n",
       "Q 1394 384 1728 384 \n",
       "Q 2125 384 2398 659 \n",
       "Q 2672 934 2672 1341 \n",
       "Q 2672 1728 2419 1979 \n",
       "Q 2166 2231 1775 2231 \n",
       "Q 1616 2231 1378 2169 \n",
       "L 1441 2663 \n",
       "Q 1497 2656 1531 2656 \n",
       "Q 1891 2656 2178 2843 \n",
       "Q 2466 3031 2466 3422 \n",
       "Q 2466 3731 2256 3934 \n",
       "Q 2047 4138 1716 4138 \n",
       "Q 1388 4138 1169 3931 \n",
       "Q 950 3725 888 3313 \n",
       "L 325 3413 \n",
       "Q 428 3978 793 4289 \n",
       "Q 1159 4600 1703 4600 \n",
       "Q 2078 4600 2393 4439 \n",
       "Q 2709 4278 2876 4000 \n",
       "Q 3044 3722 3044 3409 \n",
       "Q 3044 3113 2884 2869 \n",
       "Q 2725 2625 2413 2481 \n",
       "Q 2819 2388 3044 2092 \n",
       "Q 3269 1797 3269 1353 \n",
       "Q 3269 753 2831 336 \n",
       "Q 2394 -81 1725 -81 \n",
       "Q 1122 -81 723 278 \n",
       "Q 325 638 269 1209 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-2212\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"141.796875\"/>\n",
       "       <use xlink:href=\"#ArialMT-33\" x=\"197.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 123.983433 312.12 \n",
       "L 123.983433 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\"/>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −0.02 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(111.332652 326.277812) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-32\" d=\"M 3222 541 \n",
       "L 3222 0 \n",
       "L 194 0 \n",
       "Q 188 203 259 391 \n",
       "Q 375 700 629 1000 \n",
       "Q 884 1300 1366 1694 \n",
       "Q 2113 2306 2375 2664 \n",
       "Q 2638 3022 2638 3341 \n",
       "Q 2638 3675 2398 3904 \n",
       "Q 2159 4134 1775 4134 \n",
       "Q 1369 4134 1125 3890 \n",
       "Q 881 3647 878 3216 \n",
       "L 300 3275 \n",
       "Q 359 3922 746 4261 \n",
       "Q 1134 4600 1788 4600 \n",
       "Q 2447 4600 2831 4234 \n",
       "Q 3216 3869 3216 3328 \n",
       "Q 3216 3053 3103 2787 \n",
       "Q 2991 2522 2730 2228 \n",
       "Q 2469 1934 1863 1422 \n",
       "Q 1356 997 1212 845 \n",
       "Q 1069 694 975 541 \n",
       "L 3222 541 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-2212\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"141.796875\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"197.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 189.949334 312.12 \n",
       "L 189.949334 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\"/>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- −0.01 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(177.298552 326.277812) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-31\" d=\"M 2384 0 \n",
       "L 1822 0 \n",
       "L 1822 3584 \n",
       "Q 1619 3391 1289 3197 \n",
       "Q 959 3003 697 2906 \n",
       "L 697 3450 \n",
       "Q 1169 3672 1522 3987 \n",
       "Q 1875 4303 2022 4600 \n",
       "L 2384 4600 \n",
       "L 2384 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-2212\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"141.796875\"/>\n",
       "       <use xlink:href=\"#ArialMT-31\" x=\"197.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 255.915234 312.12 \n",
       "L 255.915234 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\"/>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.00 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(246.184765 326.277812) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 321.881134 312.12 \n",
       "L 321.881134 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\"/>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.01 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(312.150665 326.277812) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-31\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 387.847034 312.12 \n",
       "L 387.847034 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\"/>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.02 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(378.116566 326.277812) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 453.812935 312.12 \n",
       "L 453.812935 7.2 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\"/>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.03 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(444.082466 326.277812) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-33\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 39.501563 297.052864 \n",
       "L 485.901562 297.052864 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\"/>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −0.03 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(7.2 300.63177) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-2212\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"141.796875\"/>\n",
       "       <use xlink:href=\"#ArialMT-33\" x=\"197.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 39.501563 253.442384 \n",
       "L 485.901562 253.442384 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\"/>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −0.02 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(7.2 257.021291) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-2212\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"141.796875\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"197.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 39.501563 209.831905 \n",
       "L 485.901562 209.831905 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\"/>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- −0.01 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(7.2 213.410811) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-2212\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"141.796875\"/>\n",
       "       <use xlink:href=\"#ArialMT-31\" x=\"197.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 39.501563 166.221426 \n",
       "L 485.901562 166.221426 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\"/>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.00 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(13.040625 169.800332) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 39.501563 122.610946 \n",
       "L 485.901562 122.610946 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\"/>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.01 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(13.040625 126.189853) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-31\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 39.501563 79.000467 \n",
       "L 485.901562 79.000467 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\"/>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.02 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(13.040625 82.579373) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 39.501563 35.389988 \n",
       "L 485.901562 35.389988 \n",
       "\" clip-path=\"url(#p60750c91f4)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\"/>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.03 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(13.040625 38.968894) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-33\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"ma5833e5557\" d=\"M 0 3.5 \n",
       "C 0.928211 3.5 1.81853 3.131218 2.474874 2.474874 \n",
       "C 3.131218 1.81853 3.5 0.928211 3.5 0 \n",
       "C 3.5 -0.928211 3.131218 -1.81853 2.474874 -2.474874 \n",
       "C 1.81853 -3.131218 0.928211 -3.5 0 -3.5 \n",
       "C -0.928211 -3.5 -1.81853 -3.131218 -2.474874 -2.474874 \n",
       "C -3.131218 -1.81853 -3.5 -0.928211 -3.5 0 \n",
       "C -3.5 0.928211 -3.131218 1.81853 -2.474874 2.474874 \n",
       "C -1.81853 3.131218 -0.928211 3.5 0 3.5 \n",
       "z\n",
       "\" style=\"stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p60750c91f4)\">\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"87.624537\" y=\"101.826285\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"377.189466\" y=\"119.047537\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"278.65768\" y=\"251.648651\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"342.32767\" y=\"231.753758\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"465.610653\" y=\"91.20529\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"428.431282\" y=\"298.26\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"233.755425\" y=\"163.940999\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"168.343085\" y=\"145.217685\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"216.657496\" y=\"264.947993\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"59.792472\" y=\"130.627689\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"82.197125\" y=\"172.835103\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"217.694205\" y=\"266.009071\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"183.695972\" y=\"68.719934\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "     <use xlink:href=\"#ma5833e5557\" x=\"440.836284\" y=\"21.06\" style=\"fill: #4c72b0; stroke: #4c72b0; stroke-width: 0.3\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 39.501563 312.12 \n",
       "L 39.501563 7.2 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 485.901562 312.12 \n",
       "L 485.901562 7.2 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 39.501563 312.12 \n",
       "L 485.901562 312.12 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 39.501563 7.2 \n",
       "L 485.901562 7.2 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- jumps -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(87.624537 101.826285) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6a\" d=\"M 419 3928 \n",
       "L 419 4581 \n",
       "L 981 4581 \n",
       "L 981 3928 \n",
       "L 419 3928 \n",
       "z\n",
       "M -294 -1288 \n",
       "L -188 -809 \n",
       "Q -19 -853 78 -853 \n",
       "Q 250 -853 334 -739 \n",
       "Q 419 -625 419 -169 \n",
       "L 419 3319 \n",
       "L 981 3319 \n",
       "L 981 -181 \n",
       "Q 981 -794 822 -1034 \n",
       "Q 619 -1347 147 -1347 \n",
       "Q -81 -1347 -294 -1288 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-75\" d=\"M 2597 0 \n",
       "L 2597 488 \n",
       "Q 2209 -75 1544 -75 \n",
       "Q 1250 -75 995 37 \n",
       "Q 741 150 617 320 \n",
       "Q 494 491 444 738 \n",
       "Q 409 903 409 1263 \n",
       "L 409 3319 \n",
       "L 972 3319 \n",
       "L 972 1478 \n",
       "Q 972 1038 1006 884 \n",
       "Q 1059 663 1231 536 \n",
       "Q 1403 409 1656 409 \n",
       "Q 1909 409 2131 539 \n",
       "Q 2353 669 2445 892 \n",
       "Q 2538 1116 2538 1541 \n",
       "L 2538 3319 \n",
       "L 3100 3319 \n",
       "L 3100 0 \n",
       "L 2597 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6d\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 925 3319 \n",
       "L 925 2853 \n",
       "Q 1081 3097 1340 3245 \n",
       "Q 1600 3394 1931 3394 \n",
       "Q 2300 3394 2536 3241 \n",
       "Q 2772 3088 2869 2813 \n",
       "Q 3263 3394 3894 3394 \n",
       "Q 4388 3394 4653 3120 \n",
       "Q 4919 2847 4919 2278 \n",
       "L 4919 0 \n",
       "L 4359 0 \n",
       "L 4359 2091 \n",
       "Q 4359 2428 4304 2576 \n",
       "Q 4250 2725 4106 2815 \n",
       "Q 3963 2906 3769 2906 \n",
       "Q 3419 2906 3187 2673 \n",
       "Q 2956 2441 2956 1928 \n",
       "L 2956 0 \n",
       "L 2394 0 \n",
       "L 2394 2156 \n",
       "Q 2394 2531 2256 2718 \n",
       "Q 2119 2906 1806 2906 \n",
       "Q 1569 2906 1367 2781 \n",
       "Q 1166 2656 1075 2415 \n",
       "Q 984 2175 984 1722 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272 \n",
       "L 422 3319 \n",
       "L 934 3319 \n",
       "L 934 2888 \n",
       "Q 1116 3141 1344 3267 \n",
       "Q 1572 3394 1897 3394 \n",
       "Q 2322 3394 2647 3175 \n",
       "Q 2972 2956 3137 2557 \n",
       "Q 3303 2159 3303 1684 \n",
       "Q 3303 1175 3120 767 \n",
       "Q 2938 359 2589 142 \n",
       "Q 2241 -75 1856 -75 \n",
       "Q 1575 -75 1351 44 \n",
       "Q 1128 163 984 344 \n",
       "L 984 -1272 \n",
       "L 422 -1272 \n",
       "z\n",
       "M 931 1641 \n",
       "Q 931 1000 1190 694 \n",
       "Q 1450 388 1819 388 \n",
       "Q 2194 388 2461 705 \n",
       "Q 2728 1022 2728 1688 \n",
       "Q 2728 2322 2467 2637 \n",
       "Q 2206 2953 1844 2953 \n",
       "Q 1484 2953 1207 2617 \n",
       "Q 931 2281 931 1641 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-73\" d=\"M 197 991 \n",
       "L 753 1078 \n",
       "Q 800 744 1014 566 \n",
       "Q 1228 388 1613 388 \n",
       "Q 2000 388 2187 545 \n",
       "Q 2375 703 2375 916 \n",
       "Q 2375 1106 2209 1216 \n",
       "Q 2094 1291 1634 1406 \n",
       "Q 1016 1563 777 1677 \n",
       "Q 538 1791 414 1992 \n",
       "Q 291 2194 291 2438 \n",
       "Q 291 2659 392 2848 \n",
       "Q 494 3038 669 3163 \n",
       "Q 800 3259 1026 3326 \n",
       "Q 1253 3394 1513 3394 \n",
       "Q 1903 3394 2198 3281 \n",
       "Q 2494 3169 2634 2976 \n",
       "Q 2775 2784 2828 2463 \n",
       "L 2278 2388 \n",
       "Q 2241 2644 2061 2787 \n",
       "Q 1881 2931 1553 2931 \n",
       "Q 1166 2931 1000 2803 \n",
       "Q 834 2675 834 2503 \n",
       "Q 834 2394 903 2306 \n",
       "Q 972 2216 1119 2156 \n",
       "Q 1203 2125 1616 2013 \n",
       "Q 2213 1853 2448 1751 \n",
       "Q 2684 1650 2818 1456 \n",
       "Q 2953 1263 2953 975 \n",
       "Q 2953 694 2789 445 \n",
       "Q 2625 197 2315 61 \n",
       "Q 2006 -75 1616 -75 \n",
       "Q 969 -75 630 194 \n",
       "Q 291 463 197 991 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6a\"/>\n",
       "     <use xlink:href=\"#ArialMT-75\" x=\"22.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-6d\" x=\"77.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-70\" x=\"161.132812\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"216.748047\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- dolphin -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(377.189466 119.047537) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-64\" d=\"M 2575 0 \n",
       "L 2575 419 \n",
       "Q 2259 -75 1647 -75 \n",
       "Q 1250 -75 917 144 \n",
       "Q 584 363 401 755 \n",
       "Q 219 1147 219 1656 \n",
       "Q 219 2153 384 2558 \n",
       "Q 550 2963 881 3178 \n",
       "Q 1213 3394 1622 3394 \n",
       "Q 1922 3394 2156 3267 \n",
       "Q 2391 3141 2538 2938 \n",
       "L 2538 4581 \n",
       "L 3097 4581 \n",
       "L 3097 0 \n",
       "L 2575 0 \n",
       "z\n",
       "M 797 1656 \n",
       "Q 797 1019 1065 703 \n",
       "Q 1334 388 1700 388 \n",
       "Q 2069 388 2326 689 \n",
       "Q 2584 991 2584 1609 \n",
       "Q 2584 2291 2321 2609 \n",
       "Q 2059 2928 1675 2928 \n",
       "Q 1300 2928 1048 2622 \n",
       "Q 797 2316 797 1656 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6f\" d=\"M 213 1659 \n",
       "Q 213 2581 725 3025 \n",
       "Q 1153 3394 1769 3394 \n",
       "Q 2453 3394 2887 2945 \n",
       "Q 3322 2497 3322 1706 \n",
       "Q 3322 1066 3130 698 \n",
       "Q 2938 331 2570 128 \n",
       "Q 2203 -75 1769 -75 \n",
       "Q 1072 -75 642 372 \n",
       "Q 213 819 213 1659 \n",
       "z\n",
       "M 791 1659 \n",
       "Q 791 1022 1069 705 \n",
       "Q 1347 388 1769 388 \n",
       "Q 2188 388 2466 706 \n",
       "Q 2744 1025 2744 1678 \n",
       "Q 2744 2294 2464 2611 \n",
       "Q 2184 2928 1769 2928 \n",
       "Q 1347 2928 1069 2612 \n",
       "Q 791 2297 791 1659 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6c\" d=\"M 409 0 \n",
       "L 409 4581 \n",
       "L 972 4581 \n",
       "L 972 0 \n",
       "L 409 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-68\" d=\"M 422 0 \n",
       "L 422 4581 \n",
       "L 984 4581 \n",
       "L 984 2938 \n",
       "Q 1378 3394 1978 3394 \n",
       "Q 2347 3394 2619 3248 \n",
       "Q 2891 3103 3008 2847 \n",
       "Q 3125 2591 3125 2103 \n",
       "L 3125 0 \n",
       "L 2563 0 \n",
       "L 2563 2103 \n",
       "Q 2563 2525 2380 2717 \n",
       "Q 2197 2909 1863 2909 \n",
       "Q 1613 2909 1392 2779 \n",
       "Q 1172 2650 1078 2428 \n",
       "Q 984 2206 984 1816 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-69\" d=\"M 425 3934 \n",
       "L 425 4581 \n",
       "L 988 4581 \n",
       "L 988 3934 \n",
       "L 425 3934 \n",
       "z\n",
       "M 425 0 \n",
       "L 425 3319 \n",
       "L 988 3319 \n",
       "L 988 0 \n",
       "L 425 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 928 3319 \n",
       "L 928 2847 \n",
       "Q 1294 3394 1984 3394 \n",
       "Q 2284 3394 2536 3286 \n",
       "Q 2788 3178 2913 3003 \n",
       "Q 3038 2828 3088 2588 \n",
       "Q 3119 2431 3119 2041 \n",
       "L 3119 0 \n",
       "L 2556 0 \n",
       "L 2556 2019 \n",
       "Q 2556 2363 2490 2533 \n",
       "Q 2425 2703 2258 2804 \n",
       "Q 2091 2906 1866 2906 \n",
       "Q 1506 2906 1245 2678 \n",
       "Q 984 2450 984 1813 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-64\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"111.230469\"/>\n",
       "     <use xlink:href=\"#ArialMT-70\" x=\"133.447266\"/>\n",
       "     <use xlink:href=\"#ArialMT-68\" x=\"189.0625\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"244.677734\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"266.894531\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- climbs -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(278.65768 251.648651) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-63\" d=\"M 2588 1216 \n",
       "L 3141 1144 \n",
       "Q 3050 572 2676 248 \n",
       "Q 2303 -75 1759 -75 \n",
       "Q 1078 -75 664 370 \n",
       "Q 250 816 250 1647 \n",
       "Q 250 2184 428 2587 \n",
       "Q 606 2991 970 3192 \n",
       "Q 1334 3394 1763 3394 \n",
       "Q 2303 3394 2647 3120 \n",
       "Q 2991 2847 3088 2344 \n",
       "L 2541 2259 \n",
       "Q 2463 2594 2264 2762 \n",
       "Q 2066 2931 1784 2931 \n",
       "Q 1359 2931 1093 2626 \n",
       "Q 828 2322 828 1663 \n",
       "Q 828 994 1084 691 \n",
       "Q 1341 388 1753 388 \n",
       "Q 2084 388 2306 591 \n",
       "Q 2528 794 2588 1216 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-62\" d=\"M 941 0 \n",
       "L 419 0 \n",
       "L 419 4581 \n",
       "L 981 4581 \n",
       "L 981 2947 \n",
       "Q 1338 3394 1891 3394 \n",
       "Q 2197 3394 2470 3270 \n",
       "Q 2744 3147 2920 2923 \n",
       "Q 3097 2700 3197 2384 \n",
       "Q 3297 2069 3297 1709 \n",
       "Q 3297 856 2875 390 \n",
       "Q 2453 -75 1863 -75 \n",
       "Q 1275 -75 941 416 \n",
       "L 941 0 \n",
       "z\n",
       "M 934 1684 \n",
       "Q 934 1088 1097 822 \n",
       "Q 1363 388 1816 388 \n",
       "Q 2184 388 2453 708 \n",
       "Q 2722 1028 2722 1663 \n",
       "Q 2722 2313 2464 2622 \n",
       "Q 2206 2931 1841 2931 \n",
       "Q 1472 2931 1203 2611 \n",
       "Q 934 2291 934 1684 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-63\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"50\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"72.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-6d\" x=\"94.433594\"/>\n",
       "     <use xlink:href=\"#ArialMT-62\" x=\"177.734375\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"233.349609\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- koala -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(342.32767 231.753758) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6b\" d=\"M 425 0 \n",
       "L 425 4581 \n",
       "L 988 4581 \n",
       "L 988 1969 \n",
       "L 2319 3319 \n",
       "L 3047 3319 \n",
       "L 1778 2088 \n",
       "L 3175 0 \n",
       "L 2481 0 \n",
       "L 1384 1697 \n",
       "L 988 1316 \n",
       "L 988 0 \n",
       "L 425 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-61\" d=\"M 2588 409 \n",
       "Q 2275 144 1986 34 \n",
       "Q 1697 -75 1366 -75 \n",
       "Q 819 -75 525 192 \n",
       "Q 231 459 231 875 \n",
       "Q 231 1119 342 1320 \n",
       "Q 453 1522 633 1644 \n",
       "Q 813 1766 1038 1828 \n",
       "Q 1203 1872 1538 1913 \n",
       "Q 2219 1994 2541 2106 \n",
       "Q 2544 2222 2544 2253 \n",
       "Q 2544 2597 2384 2738 \n",
       "Q 2169 2928 1744 2928 \n",
       "Q 1347 2928 1158 2789 \n",
       "Q 969 2650 878 2297 \n",
       "L 328 2372 \n",
       "Q 403 2725 575 2942 \n",
       "Q 747 3159 1072 3276 \n",
       "Q 1397 3394 1825 3394 \n",
       "Q 2250 3394 2515 3294 \n",
       "Q 2781 3194 2906 3042 \n",
       "Q 3031 2891 3081 2659 \n",
       "Q 3109 2516 3109 2141 \n",
       "L 3109 1391 \n",
       "Q 3109 606 3145 398 \n",
       "Q 3181 191 3288 0 \n",
       "L 2700 0 \n",
       "Q 2613 175 2588 409 \n",
       "z\n",
       "M 2541 1666 \n",
       "Q 2234 1541 1622 1453 \n",
       "Q 1275 1403 1131 1340 \n",
       "Q 988 1278 909 1158 \n",
       "Q 831 1038 831 891 \n",
       "Q 831 666 1001 516 \n",
       "Q 1172 366 1500 366 \n",
       "Q 1825 366 2078 508 \n",
       "Q 2331 650 2450 897 \n",
       "Q 2541 1088 2541 1459 \n",
       "L 2541 1666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6b\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"50\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"105.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"161.230469\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"183.447266\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- gallops -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(465.610653 91.20529) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275 \n",
       "L 866 -356 \n",
       "Q 900 -609 1056 -725 \n",
       "Q 1266 -881 1628 -881 \n",
       "Q 2019 -881 2231 -725 \n",
       "Q 2444 -569 2519 -288 \n",
       "Q 2563 -116 2559 434 \n",
       "Q 2191 0 1641 0 \n",
       "Q 956 0 581 494 \n",
       "Q 206 988 206 1678 \n",
       "Q 206 2153 378 2554 \n",
       "Q 550 2956 876 3175 \n",
       "Q 1203 3394 1644 3394 \n",
       "Q 2231 3394 2613 2919 \n",
       "L 2613 3319 \n",
       "L 3131 3319 \n",
       "L 3131 450 \n",
       "Q 3131 -325 2973 -648 \n",
       "Q 2816 -972 2473 -1159 \n",
       "Q 2131 -1347 1631 -1347 \n",
       "Q 1038 -1347 672 -1080 \n",
       "Q 306 -813 319 -275 \n",
       "z\n",
       "M 784 1719 \n",
       "Q 784 1066 1043 766 \n",
       "Q 1303 466 1694 466 \n",
       "Q 2081 466 2343 764 \n",
       "Q 2606 1063 2606 1700 \n",
       "Q 2606 2309 2336 2618 \n",
       "Q 2066 2928 1684 2928 \n",
       "Q 1309 2928 1046 2623 \n",
       "Q 784 2319 784 1719 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-67\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"55.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"111.230469\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"133.447266\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"155.664062\"/>\n",
       "     <use xlink:href=\"#ArialMT-70\" x=\"211.279297\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"266.894531\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- horse -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(428.431282 298.26) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-72\" d=\"M 416 0 \n",
       "L 416 3319 \n",
       "L 922 3319 \n",
       "L 922 2816 \n",
       "Q 1116 3169 1280 3281 \n",
       "Q 1444 3394 1641 3394 \n",
       "Q 1925 3394 2219 3213 \n",
       "L 2025 2691 \n",
       "Q 1819 2813 1613 2813 \n",
       "Q 1428 2813 1281 2702 \n",
       "Q 1134 2591 1072 2394 \n",
       "Q 978 2094 978 1738 \n",
       "L 978 0 \n",
       "L 416 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069 \n",
       "L 3275 997 \n",
       "Q 3138 488 2766 206 \n",
       "Q 2394 -75 1816 -75 \n",
       "Q 1088 -75 661 373 \n",
       "Q 234 822 234 1631 \n",
       "Q 234 2469 665 2931 \n",
       "Q 1097 3394 1784 3394 \n",
       "Q 2450 3394 2872 2941 \n",
       "Q 3294 2488 3294 1666 \n",
       "Q 3294 1616 3291 1516 \n",
       "L 816 1516 \n",
       "Q 847 969 1125 678 \n",
       "Q 1403 388 1819 388 \n",
       "Q 2128 388 2347 550 \n",
       "Q 2566 713 2694 1069 \n",
       "z\n",
       "M 847 1978 \n",
       "L 2700 1978 \n",
       "Q 2663 2397 2488 2606 \n",
       "Q 2219 2931 1791 2931 \n",
       "Q 1403 2931 1139 2672 \n",
       "Q 875 2413 847 1978 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-68\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"111.230469\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"144.53125\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"194.53125\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- swims -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(233.755425 163.940999) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-77\" d=\"M 1034 0 \n",
       "L 19 3319 \n",
       "L 600 3319 \n",
       "L 1128 1403 \n",
       "L 1325 691 \n",
       "Q 1338 744 1497 1375 \n",
       "L 2025 3319 \n",
       "L 2603 3319 \n",
       "L 3100 1394 \n",
       "L 3266 759 \n",
       "L 3456 1400 \n",
       "L 4025 3319 \n",
       "L 4572 3319 \n",
       "L 3534 0 \n",
       "L 2950 0 \n",
       "L 2422 1988 \n",
       "L 2294 2553 \n",
       "L 1622 0 \n",
       "L 1034 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-73\"/>\n",
       "     <use xlink:href=\"#ArialMT-77\" x=\"50\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"122.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-6d\" x=\"144.433594\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"227.734375\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- fish -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(168.343085 145.217685) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-66\" d=\"M 556 0 \n",
       "L 556 2881 \n",
       "L 59 2881 \n",
       "L 59 3319 \n",
       "L 556 3319 \n",
       "L 556 3672 \n",
       "Q 556 4006 616 4169 \n",
       "Q 697 4388 901 4523 \n",
       "Q 1106 4659 1475 4659 \n",
       "Q 1713 4659 2000 4603 \n",
       "L 1916 4113 \n",
       "Q 1741 4144 1584 4144 \n",
       "Q 1328 4144 1222 4034 \n",
       "Q 1116 3925 1116 3625 \n",
       "L 1116 3319 \n",
       "L 1763 3319 \n",
       "L 1763 2881 \n",
       "L 1116 2881 \n",
       "L 1116 0 \n",
       "L 556 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-66\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"50\"/>\n",
       "     <use xlink:href=\"#ArialMT-68\" x=\"100\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_23\">\n",
       "    <!-- sings -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(216.657496 264.947993) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#ArialMT-73\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"50\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"72.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"127.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"183.447266\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_24\">\n",
       "    <!-- bird -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(59.792472 130.627689) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#ArialMT-62\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"55.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"77.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-64\" x=\"111.132812\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_25\">\n",
       "    <!-- meows -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(82.197125 172.835103) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#ArialMT-6d\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"83.300781\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"138.916016\"/>\n",
       "     <use xlink:href=\"#ArialMT-77\" x=\"194.53125\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"266.748047\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_26\">\n",
       "    <!-- cat -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(217.694205 266.009071) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-74\" d=\"M 1650 503 \n",
       "L 1731 6 \n",
       "Q 1494 -44 1306 -44 \n",
       "Q 1000 -44 831 53 \n",
       "Q 663 150 594 308 \n",
       "Q 525 466 525 972 \n",
       "L 525 2881 \n",
       "L 113 2881 \n",
       "L 113 3319 \n",
       "L 525 3319 \n",
       "L 525 4141 \n",
       "L 1084 4478 \n",
       "L 1084 3319 \n",
       "L 1650 3319 \n",
       "L 1650 2881 \n",
       "L 1084 2881 \n",
       "L 1084 941 \n",
       "Q 1084 700 1114 631 \n",
       "Q 1144 563 1211 522 \n",
       "Q 1278 481 1403 481 \n",
       "Q 1497 481 1650 503 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-63\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"50\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"105.615234\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_27\">\n",
       "    <!-- barks -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(183.695972 68.719934) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#ArialMT-62\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"55.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"111.230469\"/>\n",
       "     <use xlink:href=\"#ArialMT-6b\" x=\"144.53125\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"194.53125\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_28\">\n",
       "    <!-- dog -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(440.836284 21.06) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#ArialMT-64\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\"/>\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p60750c91f4\">\n",
       "   <rect x=\"39.501563\" y=\"7.2\" width=\"446.4\" height=\"304.92\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = list(model.wv.key_to_index)\n",
    "embeddings = model.wv[words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Simple Use Case: Finding Similar Words\n",
    "\n",
    "Let's find words that are similar to \"dog\" using our trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 0.17697495222091675), ('gallops', 0.14551663398742676), ('koala', 0.1332203447818756), ('climbs', 0.07737936824560165), ('sings', 0.06651155650615692)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('horse', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('climbs', 0.16171227395534515), ('meows', 0.0615423284471035), ('gallops', 0.04773150756955147), ('koala', 0.04535398632287979), ('jumps', 0.037129852920770645)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('swims', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return the top 5 words that are most similar to \"dog\" based on the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the simple use case of finding words similar to \"dog\":\n",
    "- \"gallops\" is the most similar with a similarity score of approximately 0.258.\n",
    "- \"dolphin\" is the next most similar with a similarity score of approximately 0.142.\n",
    "- ... and so on.\n",
    "\n",
    "For instance, in the figure, you might notice some clustering or proximity between words that have related meanings or contexts in the training data. However, it's worth noting that the quality and interpretability of embeddings heavily depend on the amount and quality of training data. In this case, our dataset was very small, so the embeddings might not capture nuanced semantic relationships as effectively as they would with a larger, more diverse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Transformer architecture, used by GPT, embeddings are used to represent words in a dense vector space, as we discussed. Beyond these word embeddings, the Transformer introduces additional embeddings to facilitate the self-attention mechanism: the Query (Q), Key (K), and Value (V) matrices.\n",
    "\n",
    "### Meaning of Q, K, and V:\n",
    "\n",
    "1. **Query (Q)**: Represents the current word for which we want to determine attention scores. It's like asking: \"For this word, how much should I attend to other words in the sequence?\"\n",
    "2. **Key (K)**: Represents all the other words in terms of determining their relevance to the current word (the one represented by the query). It's the counterpart to the query in the attention calculation.\n",
    "3. **Value (V)**: Contains the information from the input words that we actually want to sum up in the attention mechanism. Once we've determined how much to \"attend\" to each word (using Q and K), we use the V vectors to compute the weighted sum.\n",
    "\n",
    "### How are Q, K, and V derived?\n",
    "\n",
    "For each word in the input, Q, K, and V are derived by multiplying the word's embedding by three weight matrices, \\(W_Q\\), \\(W_K\\), and \\(W_V\\), respectively. These weight matrices are learned during training.\n",
    "\n",
    "$$\n",
    "Q = \\text{Embedding} \\times W_Q\n",
    "$$\n",
    "$$\n",
    "K = \\text{Embedding} \\times W_K\n",
    "$$\n",
    "$$\n",
    "V = \\text{Embedding} \\times W_V\n",
    "$$\n",
    "\n",
    "### Python Example:\n",
    "\n",
    "Let's consider a simple example with small embeddings and weight matrices to compute Q, K, and V for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.39, 0.46, 0.37]]),\n",
       " array([[0.46, 0.37, 0.54]]),\n",
       " array([[0.4 , 0.56, 0.23]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a simple word embedding for the word \"hello\"\n",
    "embedding_hello = np.array([[0.1, 0.3, 0.5]])\n",
    "\n",
    "# Define weight matrices for Q, K, and V\n",
    "W_Q = np.array([[0.2, 0.8, 0.4],\n",
    "                [0.4, 0.1, 0.6],\n",
    "                [0.5, 0.7, 0.3]])\n",
    "\n",
    "W_K = np.array([[0.5, 0.3, 0.6],\n",
    "                [0.7, 0.8, 0.1],\n",
    "                [0.4, 0.2, 0.9]])\n",
    "\n",
    "W_V = np.array([[0.1, 0.6, 0.4],\n",
    "                [0.8, 0.5, 0.3],\n",
    "                [0.3, 0.7, 0.2]])\n",
    "\n",
    "# Compute Q, K, and V for the word \"hello\"\n",
    "Q_hello = np.dot(embedding_hello, W_Q)\n",
    "K_hello = np.dot(embedding_hello, W_K)\n",
    "V_hello = np.dot(embedding_hello, W_V)\n",
    "\n",
    "Q_hello, K_hello, V_hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the computed matrices for the word \"hello\":\n",
    "\n",
    "- $Q$ (Query) for \"hello\": \\([0.39, 0.46, 0.37]\\)\n",
    "- $K$ (Key) for \"hello\": \\([0.46, 0.37, 0.54]\\)\n",
    "- $V$ (Value) for \"hello\": \\([0.4, 0.56, 0.23]\\)\n",
    "\n",
    "In this simple example, these matrices are the result of multiplying the embedding for \"hello\" by the respective weight matrices $W_Q$, $W_K$, and $W_V$. In a real Transformer model, these weight matrices would be learned during training to optimize the self-attention mechanism.\n",
    "\n",
    "Keep in mind that in practice, these embeddings and matrices would be much larger (often with dimensions in the hundreds or even thousands) and would be used in combination with embeddings from other words in the sequence to compute the self-attention output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and Attention: A Simplified Overview\n",
    "\n",
    "1. **Transformers**: Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They are primarily used for sequence-to-sequence tasks like language translation and text summarization. The core idea behind transformers is the self-attention mechanism.\n",
    "\n",
    "2. **Attention**: At a high level, attention allows the model to focus on different parts of the input sequence when producing an output. The intuition is that not all parts of the input sequence are equally relevant for a given output. Attention provides a weighted sum of the input sequence based on their relevance.\n",
    "\n",
    "### Simple Illustrative Example\n",
    "\n",
    "Imagine we have the sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "Our goal is to understand which words in the sentence are most \"relevant\" or \"attended to\" when trying to understand the word \"sat\".\n",
    "\n",
    "Let's assign some made-up attention scores to each word with respect to the word \"sat\":\n",
    "\n",
    "```\n",
    "The: 0.1\n",
    "cat: 0.3\n",
    "sat: 0.1\n",
    "on: 0.2\n",
    "the: 0.1\n",
    "mat: 0.2\n",
    "```\n",
    "\n",
    "These attention scores add up to 1 and represent the \"relevance\" of each word in understanding the word \"sat\". For example, \"cat\" has a higher relevance (0.3) than \"The\" (0.1) for understanding \"sat\".\n",
    "\n",
    "Let's visualize these attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"501.320781pt\" height=\"350.054688pt\" viewBox=\"0 0 501.320781 350.054688\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-25T04:16:25.757242</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.8.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 350.054688 \n",
       "L 501.320781 350.054688 \n",
       "L 501.320781 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 47.720781 326.709375 \n",
       "L 494.120781 326.709375 \n",
       "L 494.120781 21.789375 \n",
       "L 47.720781 21.789375 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 95.999151 326.709375 \n",
       "L 95.999151 21.789375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\"/>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- The -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(87.384307 340.867188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-54\" d=\"M 1659 0 \n",
       "L 1659 4041 \n",
       "L 150 4041 \n",
       "L 150 4581 \n",
       "L 3781 4581 \n",
       "L 3781 4041 \n",
       "L 2266 4041 \n",
       "L 2266 0 \n",
       "L 1659 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-68\" d=\"M 422 0 \n",
       "L 422 4581 \n",
       "L 984 4581 \n",
       "L 984 2938 \n",
       "Q 1378 3394 1978 3394 \n",
       "Q 2347 3394 2619 3248 \n",
       "Q 2891 3103 3008 2847 \n",
       "Q 3125 2591 3125 2103 \n",
       "L 3125 0 \n",
       "L 2563 0 \n",
       "L 2563 2103 \n",
       "Q 2563 2525 2380 2717 \n",
       "Q 2197 2909 1863 2909 \n",
       "Q 1613 2909 1392 2779 \n",
       "Q 1172 2650 1078 2428 \n",
       "Q 984 2206 984 1816 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-65\" d=\"M 2694 1069 \n",
       "L 3275 997 \n",
       "Q 3138 488 2766 206 \n",
       "Q 2394 -75 1816 -75 \n",
       "Q 1088 -75 661 373 \n",
       "Q 234 822 234 1631 \n",
       "Q 234 2469 665 2931 \n",
       "Q 1097 3394 1784 3394 \n",
       "Q 2450 3394 2872 2941 \n",
       "Q 3294 2488 3294 1666 \n",
       "Q 3294 1616 3291 1516 \n",
       "L 816 1516 \n",
       "Q 847 969 1125 678 \n",
       "Q 1403 388 1819 388 \n",
       "Q 2128 388 2347 550 \n",
       "Q 2566 713 2694 1069 \n",
       "z\n",
       "M 847 1978 \n",
       "L 2700 1978 \n",
       "Q 2663 2397 2488 2606 \n",
       "Q 2219 2931 1791 2931 \n",
       "Q 1403 2931 1139 2672 \n",
       "Q 875 2413 847 1978 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-54\"/>\n",
       "       <use xlink:href=\"#ArialMT-68\" x=\"61.083984\"/>\n",
       "       <use xlink:href=\"#ArialMT-65\" x=\"116.699219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 165.967803 326.709375 \n",
       "L 165.967803 21.789375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\"/>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- cat -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(159.298272 340.867188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-63\" d=\"M 2588 1216 \n",
       "L 3141 1144 \n",
       "Q 3050 572 2676 248 \n",
       "Q 2303 -75 1759 -75 \n",
       "Q 1078 -75 664 370 \n",
       "Q 250 816 250 1647 \n",
       "Q 250 2184 428 2587 \n",
       "Q 606 2991 970 3192 \n",
       "Q 1334 3394 1763 3394 \n",
       "Q 2303 3394 2647 3120 \n",
       "Q 2991 2847 3088 2344 \n",
       "L 2541 2259 \n",
       "Q 2463 2594 2264 2762 \n",
       "Q 2066 2931 1784 2931 \n",
       "Q 1359 2931 1093 2626 \n",
       "Q 828 2322 828 1663 \n",
       "Q 828 994 1084 691 \n",
       "Q 1341 388 1753 388 \n",
       "Q 2084 388 2306 591 \n",
       "Q 2528 794 2588 1216 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-61\" d=\"M 2588 409 \n",
       "Q 2275 144 1986 34 \n",
       "Q 1697 -75 1366 -75 \n",
       "Q 819 -75 525 192 \n",
       "Q 231 459 231 875 \n",
       "Q 231 1119 342 1320 \n",
       "Q 453 1522 633 1644 \n",
       "Q 813 1766 1038 1828 \n",
       "Q 1203 1872 1538 1913 \n",
       "Q 2219 1994 2541 2106 \n",
       "Q 2544 2222 2544 2253 \n",
       "Q 2544 2597 2384 2738 \n",
       "Q 2169 2928 1744 2928 \n",
       "Q 1347 2928 1158 2789 \n",
       "Q 969 2650 878 2297 \n",
       "L 328 2372 \n",
       "Q 403 2725 575 2942 \n",
       "Q 747 3159 1072 3276 \n",
       "Q 1397 3394 1825 3394 \n",
       "Q 2250 3394 2515 3294 \n",
       "Q 2781 3194 2906 3042 \n",
       "Q 3031 2891 3081 2659 \n",
       "Q 3109 2516 3109 2141 \n",
       "L 3109 1391 \n",
       "Q 3109 606 3145 398 \n",
       "Q 3181 191 3288 0 \n",
       "L 2700 0 \n",
       "Q 2613 175 2588 409 \n",
       "z\n",
       "M 2541 1666 \n",
       "Q 2234 1541 1622 1453 \n",
       "Q 1275 1403 1131 1340 \n",
       "Q 988 1278 909 1158 \n",
       "Q 831 1038 831 891 \n",
       "Q 831 666 1001 516 \n",
       "Q 1172 366 1500 366 \n",
       "Q 1825 366 2078 508 \n",
       "Q 2331 650 2450 897 \n",
       "Q 2541 1088 2541 1459 \n",
       "L 2541 1666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-74\" d=\"M 1650 503 \n",
       "L 1731 6 \n",
       "Q 1494 -44 1306 -44 \n",
       "Q 1000 -44 831 53 \n",
       "Q 663 150 594 308 \n",
       "Q 525 466 525 972 \n",
       "L 525 2881 \n",
       "L 113 2881 \n",
       "L 113 3319 \n",
       "L 525 3319 \n",
       "L 525 4141 \n",
       "L 1084 4478 \n",
       "L 1084 3319 \n",
       "L 1650 3319 \n",
       "L 1650 2881 \n",
       "L 1084 2881 \n",
       "L 1084 941 \n",
       "Q 1084 700 1114 631 \n",
       "Q 1144 563 1211 522 \n",
       "Q 1278 481 1403 481 \n",
       "Q 1497 481 1650 503 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-63\"/>\n",
       "       <use xlink:href=\"#ArialMT-61\" x=\"50\"/>\n",
       "       <use xlink:href=\"#ArialMT-74\" x=\"105.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 235.936455 326.709375 \n",
       "L 235.936455 21.789375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\"/>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- sat -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(229.266924 340.867188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-73\" d=\"M 197 991 \n",
       "L 753 1078 \n",
       "Q 800 744 1014 566 \n",
       "Q 1228 388 1613 388 \n",
       "Q 2000 388 2187 545 \n",
       "Q 2375 703 2375 916 \n",
       "Q 2375 1106 2209 1216 \n",
       "Q 2094 1291 1634 1406 \n",
       "Q 1016 1563 777 1677 \n",
       "Q 538 1791 414 1992 \n",
       "Q 291 2194 291 2438 \n",
       "Q 291 2659 392 2848 \n",
       "Q 494 3038 669 3163 \n",
       "Q 800 3259 1026 3326 \n",
       "Q 1253 3394 1513 3394 \n",
       "Q 1903 3394 2198 3281 \n",
       "Q 2494 3169 2634 2976 \n",
       "Q 2775 2784 2828 2463 \n",
       "L 2278 2388 \n",
       "Q 2241 2644 2061 2787 \n",
       "Q 1881 2931 1553 2931 \n",
       "Q 1166 2931 1000 2803 \n",
       "Q 834 2675 834 2503 \n",
       "Q 834 2394 903 2306 \n",
       "Q 972 2216 1119 2156 \n",
       "Q 1203 2125 1616 2013 \n",
       "Q 2213 1853 2448 1751 \n",
       "Q 2684 1650 2818 1456 \n",
       "Q 2953 1263 2953 975 \n",
       "Q 2953 694 2789 445 \n",
       "Q 2625 197 2315 61 \n",
       "Q 2006 -75 1616 -75 \n",
       "Q 969 -75 630 194 \n",
       "Q 291 463 197 991 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-73\"/>\n",
       "       <use xlink:href=\"#ArialMT-61\" x=\"50\"/>\n",
       "       <use xlink:href=\"#ArialMT-74\" x=\"105.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 305.905107 326.709375 \n",
       "L 305.905107 21.789375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\"/>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- on -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(300.34417 340.867188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-6f\" d=\"M 213 1659 \n",
       "Q 213 2581 725 3025 \n",
       "Q 1153 3394 1769 3394 \n",
       "Q 2453 3394 2887 2945 \n",
       "Q 3322 2497 3322 1706 \n",
       "Q 3322 1066 3130 698 \n",
       "Q 2938 331 2570 128 \n",
       "Q 2203 -75 1769 -75 \n",
       "Q 1072 -75 642 372 \n",
       "Q 213 819 213 1659 \n",
       "z\n",
       "M 791 1659 \n",
       "Q 791 1022 1069 705 \n",
       "Q 1347 388 1769 388 \n",
       "Q 2188 388 2466 706 \n",
       "Q 2744 1025 2744 1678 \n",
       "Q 2744 2294 2464 2611 \n",
       "Q 2184 2928 1769 2928 \n",
       "Q 1347 2928 1069 2612 \n",
       "Q 791 2297 791 1659 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-6e\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 928 3319 \n",
       "L 928 2847 \n",
       "Q 1294 3394 1984 3394 \n",
       "Q 2284 3394 2536 3286 \n",
       "Q 2788 3178 2913 3003 \n",
       "Q 3038 2828 3088 2588 \n",
       "Q 3119 2431 3119 2041 \n",
       "L 3119 0 \n",
       "L 2556 0 \n",
       "L 2556 2019 \n",
       "Q 2556 2363 2490 2533 \n",
       "Q 2425 2703 2258 2804 \n",
       "Q 2091 2906 1866 2906 \n",
       "Q 1506 2906 1245 2678 \n",
       "Q 984 2450 984 1813 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-6f\"/>\n",
       "       <use xlink:href=\"#ArialMT-6e\" x=\"55.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 375.873759 326.709375 \n",
       "L 375.873759 21.789375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\"/>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- the -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(368.923759 340.867188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-74\"/>\n",
       "       <use xlink:href=\"#ArialMT-68\" x=\"27.783203\"/>\n",
       "       <use xlink:href=\"#ArialMT-65\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 445.842411 326.709375 \n",
       "L 445.842411 21.789375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\"/>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- mat -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(437.508036 340.867188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-6d\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 925 3319 \n",
       "L 925 2853 \n",
       "Q 1081 3097 1340 3245 \n",
       "Q 1600 3394 1931 3394 \n",
       "Q 2300 3394 2536 3241 \n",
       "Q 2772 3088 2869 2813 \n",
       "Q 3263 3394 3894 3394 \n",
       "Q 4388 3394 4653 3120 \n",
       "Q 4919 2847 4919 2278 \n",
       "L 4919 0 \n",
       "L 4359 0 \n",
       "L 4359 2091 \n",
       "Q 4359 2428 4304 2576 \n",
       "Q 4250 2725 4106 2815 \n",
       "Q 3963 2906 3769 2906 \n",
       "Q 3419 2906 3187 2673 \n",
       "Q 2956 2441 2956 1928 \n",
       "L 2956 0 \n",
       "L 2394 0 \n",
       "L 2394 2156 \n",
       "Q 2394 2531 2256 2718 \n",
       "Q 2119 2906 1806 2906 \n",
       "Q 1569 2906 1367 2781 \n",
       "Q 1166 2656 1075 2415 \n",
       "Q 984 2175 984 1722 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-6d\"/>\n",
       "       <use xlink:href=\"#ArialMT-61\" x=\"83.300781\"/>\n",
       "       <use xlink:href=\"#ArialMT-74\" x=\"138.916016\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 47.720781 326.709375 \n",
       "L 494.120781 326.709375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\"/>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.00 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 330.288281) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-30\" d=\"M 266 2259 \n",
       "Q 266 3072 433 3567 \n",
       "Q 600 4063 929 4331 \n",
       "Q 1259 4600 1759 4600 \n",
       "Q 2128 4600 2406 4451 \n",
       "Q 2684 4303 2865 4023 \n",
       "Q 3047 3744 3150 3342 \n",
       "Q 3253 2941 3253 2259 \n",
       "Q 3253 1453 3087 958 \n",
       "Q 2922 463 2592 192 \n",
       "Q 2263 -78 1759 -78 \n",
       "Q 1097 -78 719 397 \n",
       "Q 266 969 266 2259 \n",
       "z\n",
       "M 844 2259 \n",
       "Q 844 1131 1108 757 \n",
       "Q 1372 384 1759 384 \n",
       "Q 2147 384 2411 759 \n",
       "Q 2675 1134 2675 2259 \n",
       "Q 2675 3391 2411 3762 \n",
       "Q 2147 4134 1753 4134 \n",
       "Q 1366 4134 1134 3806 \n",
       "Q 844 3388 844 2259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-2e\" d=\"M 581 0 \n",
       "L 581 641 \n",
       "L 1222 641 \n",
       "L 1222 0 \n",
       "L 581 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 47.720781 278.309375 \n",
       "L 494.120781 278.309375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\"/>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.05 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 281.888281) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-35\" d=\"M 266 1200 \n",
       "L 856 1250 \n",
       "Q 922 819 1161 601 \n",
       "Q 1400 384 1738 384 \n",
       "Q 2144 384 2425 690 \n",
       "Q 2706 997 2706 1503 \n",
       "Q 2706 1984 2436 2262 \n",
       "Q 2166 2541 1728 2541 \n",
       "Q 1456 2541 1237 2417 \n",
       "Q 1019 2294 894 2097 \n",
       "L 366 2166 \n",
       "L 809 4519 \n",
       "L 3088 4519 \n",
       "L 3088 3981 \n",
       "L 1259 3981 \n",
       "L 1013 2750 \n",
       "Q 1425 3038 1878 3038 \n",
       "Q 2478 3038 2890 2622 \n",
       "Q 3303 2206 3303 1553 \n",
       "Q 3303 931 2941 478 \n",
       "Q 2500 -78 1738 -78 \n",
       "Q 1113 -78 717 272 \n",
       "Q 322 622 266 1200 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 47.720781 229.909375 \n",
       "L 494.120781 229.909375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\"/>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 233.488281) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-31\" d=\"M 2384 0 \n",
       "L 1822 0 \n",
       "L 1822 3584 \n",
       "Q 1619 3391 1289 3197 \n",
       "Q 959 3003 697 2906 \n",
       "L 697 3450 \n",
       "Q 1169 3672 1522 3987 \n",
       "Q 1875 4303 2022 4600 \n",
       "L 2384 4600 \n",
       "L 2384 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-31\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 47.720781 181.509375 \n",
       "L 494.120781 181.509375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\"/>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 185.088281) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-31\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 47.720781 133.109375 \n",
       "L 494.120781 133.109375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\"/>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.20 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 136.688281) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-32\" d=\"M 3222 541 \n",
       "L 3222 0 \n",
       "L 194 0 \n",
       "Q 188 203 259 391 \n",
       "Q 375 700 629 1000 \n",
       "Q 884 1300 1366 1694 \n",
       "Q 2113 2306 2375 2664 \n",
       "Q 2638 3022 2638 3341 \n",
       "Q 2638 3675 2398 3904 \n",
       "Q 2159 4134 1775 4134 \n",
       "Q 1369 4134 1125 3890 \n",
       "Q 881 3647 878 3216 \n",
       "L 300 3275 \n",
       "Q 359 3922 746 4261 \n",
       "Q 1134 4600 1788 4600 \n",
       "Q 2447 4600 2831 4234 \n",
       "Q 3216 3869 3216 3328 \n",
       "Q 3216 3053 3103 2787 \n",
       "Q 2991 2522 2730 2228 \n",
       "Q 2469 1934 1863 1422 \n",
       "Q 1356 997 1212 845 \n",
       "Q 1069 694 975 541 \n",
       "L 3222 541 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 47.720781 84.709375 \n",
       "L 494.120781 84.709375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\"/>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.25 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 88.288281) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 47.720781 36.309375 \n",
       "L 494.120781 36.309375 \n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\"/>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.30 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.259844 39.888281) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-33\" d=\"M 269 1209 \n",
       "L 831 1284 \n",
       "Q 928 806 1161 595 \n",
       "Q 1394 384 1728 384 \n",
       "Q 2125 384 2398 659 \n",
       "Q 2672 934 2672 1341 \n",
       "Q 2672 1728 2419 1979 \n",
       "Q 2166 2231 1775 2231 \n",
       "Q 1616 2231 1378 2169 \n",
       "L 1441 2663 \n",
       "Q 1497 2656 1531 2656 \n",
       "Q 1891 2656 2178 2843 \n",
       "Q 2466 3031 2466 3422 \n",
       "Q 2466 3731 2256 3934 \n",
       "Q 2047 4138 1716 4138 \n",
       "Q 1388 4138 1169 3931 \n",
       "Q 950 3725 888 3313 \n",
       "L 325 3413 \n",
       "Q 428 3978 793 4289 \n",
       "Q 1159 4600 1703 4600 \n",
       "Q 2078 4600 2393 4439 \n",
       "Q 2709 4278 2876 4000 \n",
       "Q 3044 3722 3044 3409 \n",
       "Q 3044 3113 2884 2869 \n",
       "Q 2725 2625 2413 2481 \n",
       "Q 2819 2388 3044 2092 \n",
       "Q 3269 1797 3269 1353 \n",
       "Q 3269 753 2831 336 \n",
       "Q 2394 -81 1725 -81 \n",
       "Q 1122 -81 723 278 \n",
       "Q 325 638 269 1209 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-33\" x=\"83.398438\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"139.013672\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- Attention Scores -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(15.073594 214.603047) rotate(-90) scale(0.11 -0.11)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-41\" d=\"M -9 0 \n",
       "L 1750 4581 \n",
       "L 2403 4581 \n",
       "L 4278 0 \n",
       "L 3588 0 \n",
       "L 3053 1388 \n",
       "L 1138 1388 \n",
       "L 634 0 \n",
       "L -9 0 \n",
       "z\n",
       "M 1313 1881 \n",
       "L 2866 1881 \n",
       "L 2388 3150 \n",
       "Q 2169 3728 2063 4100 \n",
       "Q 1975 3659 1816 3225 \n",
       "L 1313 1881 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-69\" d=\"M 425 3934 \n",
       "L 425 4581 \n",
       "L 988 4581 \n",
       "L 988 3934 \n",
       "L 425 3934 \n",
       "z\n",
       "M 425 0 \n",
       "L 425 3319 \n",
       "L 988 3319 \n",
       "L 988 0 \n",
       "L 425 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-53\" d=\"M 288 1472 \n",
       "L 859 1522 \n",
       "Q 900 1178 1048 958 \n",
       "Q 1197 738 1509 602 \n",
       "Q 1822 466 2213 466 \n",
       "Q 2559 466 2825 569 \n",
       "Q 3091 672 3220 851 \n",
       "Q 3350 1031 3350 1244 \n",
       "Q 3350 1459 3225 1620 \n",
       "Q 3100 1781 2813 1891 \n",
       "Q 2628 1963 1997 2114 \n",
       "Q 1366 2266 1113 2400 \n",
       "Q 784 2572 623 2826 \n",
       "Q 463 3081 463 3397 \n",
       "Q 463 3744 659 4045 \n",
       "Q 856 4347 1234 4503 \n",
       "Q 1613 4659 2075 4659 \n",
       "Q 2584 4659 2973 4495 \n",
       "Q 3363 4331 3572 4012 \n",
       "Q 3781 3694 3797 3291 \n",
       "L 3216 3247 \n",
       "Q 3169 3681 2898 3903 \n",
       "Q 2628 4125 2100 4125 \n",
       "Q 1550 4125 1298 3923 \n",
       "Q 1047 3722 1047 3438 \n",
       "Q 1047 3191 1225 3031 \n",
       "Q 1400 2872 2139 2705 \n",
       "Q 2878 2538 3153 2413 \n",
       "Q 3553 2228 3743 1945 \n",
       "Q 3934 1663 3934 1294 \n",
       "Q 3934 928 3725 604 \n",
       "Q 3516 281 3123 101 \n",
       "Q 2731 -78 2241 -78 \n",
       "Q 1619 -78 1198 103 \n",
       "Q 778 284 539 648 \n",
       "Q 300 1013 288 1472 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-72\" d=\"M 416 0 \n",
       "L 416 3319 \n",
       "L 922 3319 \n",
       "L 922 2816 \n",
       "Q 1116 3169 1280 3281 \n",
       "Q 1444 3394 1641 3394 \n",
       "Q 1925 3394 2219 3213 \n",
       "L 2025 2691 \n",
       "Q 1819 2813 1613 2813 \n",
       "Q 1428 2813 1281 2702 \n",
       "Q 1134 2591 1072 2394 \n",
       "Q 978 2094 978 1738 \n",
       "L 978 0 \n",
       "L 416 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-41\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"94.482422\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"122.265625\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"177.880859\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"233.496094\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"261.279297\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"283.496094\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"339.111328\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"394.726562\"/>\n",
       "      <use xlink:href=\"#ArialMT-53\" x=\"422.509766\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"489.208984\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"539.208984\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"594.824219\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"628.125\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"683.740234\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 68.01169 326.709375 \n",
       "L 123.986612 326.709375 \n",
       "L 123.986612 229.909375 \n",
       "L 68.01169 229.909375 \n",
       "z\n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: #4c72b0\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 137.980342 326.709375 \n",
       "L 193.955264 326.709375 \n",
       "L 193.955264 36.309375 \n",
       "L 137.980342 36.309375 \n",
       "z\n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: #4c72b0\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 207.948994 326.709375 \n",
       "L 263.923916 326.709375 \n",
       "L 263.923916 229.909375 \n",
       "L 207.948994 229.909375 \n",
       "z\n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: #4c72b0\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 277.917646 326.709375 \n",
       "L 333.892568 326.709375 \n",
       "L 333.892568 133.109375 \n",
       "L 277.917646 133.109375 \n",
       "z\n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: #4c72b0\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 347.886298 326.709375 \n",
       "L 403.86122 326.709375 \n",
       "L 403.86122 229.909375 \n",
       "L 347.886298 229.909375 \n",
       "z\n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: #4c72b0\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 417.854951 326.709375 \n",
       "L 473.829872 326.709375 \n",
       "L 473.829872 133.109375 \n",
       "L 417.854951 133.109375 \n",
       "z\n",
       "\" clip-path=\"url(#pd0684c9366)\" style=\"fill: #4c72b0\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 47.720781 326.709375 \n",
       "L 47.720781 21.789375 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 494.120781 326.709375 \n",
       "L 494.120781 21.789375 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 47.720781 326.709375 \n",
       "L 494.120781 326.709375 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 47.720781 21.789375 \n",
       "L 494.120781 21.789375 \n",
       "\" style=\"fill: none\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- Attention Scores for the word \"sat\" -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(179.622344 15.789375) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-66\" d=\"M 556 0 \n",
       "L 556 2881 \n",
       "L 59 2881 \n",
       "L 59 3319 \n",
       "L 556 3319 \n",
       "L 556 3672 \n",
       "Q 556 4006 616 4169 \n",
       "Q 697 4388 901 4523 \n",
       "Q 1106 4659 1475 4659 \n",
       "Q 1713 4659 2000 4603 \n",
       "L 1916 4113 \n",
       "Q 1741 4144 1584 4144 \n",
       "Q 1328 4144 1222 4034 \n",
       "Q 1116 3925 1116 3625 \n",
       "L 1116 3319 \n",
       "L 1763 3319 \n",
       "L 1763 2881 \n",
       "L 1116 2881 \n",
       "L 1116 0 \n",
       "L 556 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-77\" d=\"M 1034 0 \n",
       "L 19 3319 \n",
       "L 600 3319 \n",
       "L 1128 1403 \n",
       "L 1325 691 \n",
       "Q 1338 744 1497 1375 \n",
       "L 2025 3319 \n",
       "L 2603 3319 \n",
       "L 3100 1394 \n",
       "L 3266 759 \n",
       "L 3456 1400 \n",
       "L 4025 3319 \n",
       "L 4572 3319 \n",
       "L 3534 0 \n",
       "L 2950 0 \n",
       "L 2422 1988 \n",
       "L 2294 2553 \n",
       "L 1622 0 \n",
       "L 1034 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-64\" d=\"M 2575 0 \n",
       "L 2575 419 \n",
       "Q 2259 -75 1647 -75 \n",
       "Q 1250 -75 917 144 \n",
       "Q 584 363 401 755 \n",
       "Q 219 1147 219 1656 \n",
       "Q 219 2153 384 2558 \n",
       "Q 550 2963 881 3178 \n",
       "Q 1213 3394 1622 3394 \n",
       "Q 1922 3394 2156 3267 \n",
       "Q 2391 3141 2538 2938 \n",
       "L 2538 4581 \n",
       "L 3097 4581 \n",
       "L 3097 0 \n",
       "L 2575 0 \n",
       "z\n",
       "M 797 1656 \n",
       "Q 797 1019 1065 703 \n",
       "Q 1334 388 1700 388 \n",
       "Q 2069 388 2326 689 \n",
       "Q 2584 991 2584 1609 \n",
       "Q 2584 2291 2321 2609 \n",
       "Q 2059 2928 1675 2928 \n",
       "Q 1300 2928 1048 2622 \n",
       "Q 797 2316 797 1656 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-22\" d=\"M 450 2959 \n",
       "L 294 3831 \n",
       "L 294 4581 \n",
       "L 934 4581 \n",
       "L 934 3831 \n",
       "L 794 2959 \n",
       "L 450 2959 \n",
       "z\n",
       "M 1484 2959 \n",
       "L 1331 3831 \n",
       "L 1331 4581 \n",
       "L 1972 4581 \n",
       "L 1972 3831 \n",
       "L 1822 2959 \n",
       "L 1484 2959 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-41\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"66.699219\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"94.482422\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"122.265625\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"177.880859\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"233.496094\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"261.279297\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"283.496094\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"339.111328\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"394.726562\"/>\n",
       "     <use xlink:href=\"#ArialMT-53\" x=\"422.509766\"/>\n",
       "     <use xlink:href=\"#ArialMT-63\" x=\"489.208984\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"539.208984\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"594.824219\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"628.125\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"683.740234\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"733.740234\"/>\n",
       "     <use xlink:href=\"#ArialMT-66\" x=\"761.523438\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"789.306641\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"844.921875\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"878.222656\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"906.005859\"/>\n",
       "     <use xlink:href=\"#ArialMT-68\" x=\"933.789062\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"989.404297\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1045.019531\"/>\n",
       "     <use xlink:href=\"#ArialMT-77\" x=\"1072.802734\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"1145.019531\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"1200.634766\"/>\n",
       "     <use xlink:href=\"#ArialMT-64\" x=\"1233.935547\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1289.550781\"/>\n",
       "     <use xlink:href=\"#ArialMT-22\" x=\"1317.333984\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"1352.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"1402.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"1458.447266\"/>\n",
       "     <use xlink:href=\"#ArialMT-22\" x=\"1486.230469\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pd0684c9366\">\n",
       "   <rect x=\"47.720781\" y=\"21.789375\" width=\"446.4\" height=\"304.92\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "attention_scores = [0.1, 0.3, 0.1, 0.2, 0.1, 0.2]\n",
    "\n",
    "plt.bar(sentence, attention_scores)\n",
    "plt.ylabel('Attention Scores')\n",
    "plt.title('Attention Scores for the word \"sat\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Attention Scores are Computed\n",
    "\n",
    "The attention mechanism in transformers is based on the concept of **queries**, **keys**, and **values**. Each word in our input sequence (like our sentence \"The cat sat on the mat.\") gets transformed into these three representations.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. **Embedding**: First, each word in the sentence is transformed into an embedding (a dense vector representation).\n",
    "2. **Compute Q, K, V**: The embeddings are then transformed into query (Q), key (K), and value (V) representations using separate weight matrices for each transformation.\n",
    "3. **Calculate Attention Scores**: For a particular word (like \"sat\"), its query representation (Q) is used to calculate scores with all the key representations (K) of the other words in the sentence. This is usually done using a dot product.\n",
    "4. **Softmax**: The raw attention scores from the previous step are then passed through a softmax function to make them sum up to 1.\n",
    "5. **Compute Output**: The softmaxed attention scores are used to take a weighted sum of the value (V) representations. This gives the output representation for the word \"sat\".\n",
    "\n",
    "For simplicity, let's take a toy example. Imagine each word's embedding is a 2-dimensional vector, and our Q, K, V transformations are just 2x2 matrices. Let's compute attention scores for the word \"sat\" in our sentence.\n",
    "\n",
    "I'll generate some random embeddings for our words and random transformation matrices for Q, K, and V. Then, we'll compute the attention scores for the word \"sat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 0.17081296316411468,\n",
       " 'cat': 0.1705392605796768,\n",
       " 'sat': 0.15670420725071152,\n",
       " 'on': 0.16530348255137817,\n",
       " 'the': 0.17035257133704534,\n",
       " 'mat': 0.1662875151170735}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Toy embeddings for our sentence\n",
    "embeddings = {\n",
    "    \"The\": np.random.rand(2),\n",
    "    \"cat\": np.random.rand(2),\n",
    "    \"sat\": np.random.rand(2),\n",
    "    \"on\": np.random.rand(2),\n",
    "    \"the\": np.random.rand(2),\n",
    "    \"mat\": np.random.rand(2)\n",
    "}\n",
    "\n",
    "# Toy transformation matrices for Q, K, and V\n",
    "W_q = np.random.rand(2, 2)\n",
    "W_k = np.random.rand(2, 2)\n",
    "W_v = np.random.rand(2, 2)\n",
    "\n",
    "# Compute Q, K, V for each word\n",
    "Q = {word: np.dot(vec, W_q) for word, vec in embeddings.items()}\n",
    "K = {word: np.dot(vec, W_k) for word, vec in embeddings.items()}\n",
    "V = {word: np.dot(vec, W_v) for word, vec in embeddings.items()}\n",
    "\n",
    "# Compute raw attention scores for \"sat\"\n",
    "attention_raw_scores = {word: np.dot(Q[\"sat\"], vec) for word, vec in K.items()}\n",
    "\n",
    "# Softmax the scores\n",
    "attention_scores = {word: np.exp(score) / sum(np.exp(list(attention_raw_scores.values())))\n",
    "                    for word, score in attention_raw_scores.items()}\n",
    "\n",
    "attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our toy example with random embeddings and transformation matrices, here are the computed attention scores for the word \"sat\" in the sentence:\n",
    "\n",
    "```\n",
    "'The': 0.1708\n",
    "'cat': 0.1705\n",
    "'sat': 0.1567\n",
    "'on': 0.1653\n",
    "'the': 0.1703\n",
    "'mat': 0.1663\n",
    "```\n",
    "\n",
    "These scores represent the \"relevance\" of each word in understanding the context of \"sat\". In this example, all words have roughly similar attention scores because our embeddings and transformation matrices were generated randomly. In a trained transformer model, the embeddings and transformation matrices are learned from data, and the attention scores would reflect meaningful relationships between words.\n",
    "\n",
    "The final output representation for the word \"sat\" would be a weighted combination of the value (V) representations of all words, using the above attention scores as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's compute the final output representation for the word \"sat\" using the computed attention scores and the value (V) representations.\n",
    "\n",
    "The output representation for a word is calculated as a weighted sum of the value (V) representations of all words in the sentence, where the weights are the attention scores.\n",
    "\n",
    "Mathematically, for our word \"sat\", the output representation \\( \\text{output}_{\\text{sat}} \\) is given by:\n",
    "\n",
    "$$\n",
    "\\text{output}_{\\text{sat}} = \\sum_{\\text{word} \\in \\text{sentence}} \\text{attention\\_score}_{\\text{word}} \\times V_{\\text{word}}\n",
    "$$\n",
    "\n",
    "Let's compute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40970433, 0.30743988])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the output representation for \"sat\"\n",
    "output_sat = sum(attention_scores[word] * V[word] for word in sentence)\n",
    "\n",
    "output_sat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output representation for the word \"sat\" based on our toy example is approximately:\n",
    "\n",
    "$$\n",
    "\\text{output}_{\\text{sat}} = [0.4097, 0.3074]\n",
    "$$\n",
    "\n",
    "This 2-dimensional vector represents the context-aware representation of the word \"sat\" after considering the entire sentence. In a real transformer model, this representation would be much higher-dimensional (often 768 dimensions or more) and would capture rich contextual information.\n",
    "\n",
    "This context-aware representation can then be used for various downstream tasks like classification, translation, or generation, depending on the specific use case of the transformer model.\n",
    "\n",
    "To recap:\n",
    "\n",
    "1. We transformed each word into **Query (Q)**, **Key (K)**, and **Value (V)** representations.\n",
    "2. We computed attention scores for the word \"sat\" using its Q representation and the K representations of all other words.\n",
    "3. We obtained the final output representation for \"sat\" by taking a weighted sum of all V representations using the attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0.14484225168642711, 'cat': 0.17276723580854916, 'sat': 0.16604274094750116, 'on': 0.16759414126810201, 'the': 0.1372281996257353, 'mat': 0.2115254306636853}\n",
      "[1.03866402 1.14194379]\n"
     ]
    }
   ],
   "source": [
    "# Complete example\n",
    "def compute_attention_scores(sentence, embeddings, W_q, W_k, W_v):\n",
    "    # Compute Q, K, V for each word\n",
    "    Q = {word: np.dot(embeddings[word], W_q) for word in sentence}\n",
    "    K = {word: np.dot(embeddings[word], W_k) for word in sentence}\n",
    "    V = {word: np.dot(embeddings[word], W_v) for word in sentence}\n",
    "\n",
    "    # Compute raw attention scores for \"sat\"\n",
    "    attention_raw_scores = {word: np.dot(Q[\"sat\"], K[word]) for word in sentence}\n",
    "\n",
    "    # Softmax the scores\n",
    "    attention_scores = {\n",
    "        word: np.exp(score) / sum(np.exp(list(attention_raw_scores.values())))\n",
    "        for word, score in attention_raw_scores.items()\n",
    "    }\n",
    "\n",
    "    # Compute the output representation for \"sat\"\n",
    "    output_sat = np.sum([attention_scores[word] * V[word] for word in sentence], axis=0)\n",
    "    \n",
    "    return attention_scores, output_sat\n",
    "\n",
    "# Manually defined 2-dimensional embeddings for the sentence \"The cat sat on the mat.\"\n",
    "embeddings = {\n",
    "    \"The\": np.array([0.5, 0.2]),\n",
    "    \"cat\": np.array([0.9, 0.3]),\n",
    "    \"sat\": np.array([0.1, 0.8]),\n",
    "    \"on\": np.array([0.4, 0.6]),\n",
    "    \"the\": np.array([0.2, 0.3]),\n",
    "    \"mat\": np.array([0.7, 0.9])\n",
    "}\n",
    "\n",
    "# Manually defined 2x2 transformation matrices for Q, K, and V\n",
    "W_q = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "W_k = np.array([[0.5, 0.6], [0.7, 0.8]])\n",
    "W_v = np.array([[0.9, 1.0], [1.1, 1.2]])\n",
    "\n",
    "attention_scores, output_sat = compute_attention_scores(sentence, embeddings, W_q, W_k, W_v)\n",
    "print(attention_scores)\n",
    "print(output_sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Python is a first-class citizen in the AI world. The AI world is great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 23334, 1110, 170, 1148, 118, 1705, 7888, 1107, 1103, 19016, 1362, 119, 1109, 19016, 1362, 1110, 1632, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(t, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       "array([[  101, 23334,  1110,   170,  1148,   118,  1705,  7888,  1107,\n",
       "         1103, 19016,  1362,   119,  1109, 19016,  1362,  1110,  1632,\n",
       "          119,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "      dtype=int32)>}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = ['It is hot in here.', 'This is cold.', 'What a great weather.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(nt, padding=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n",
       "array([[ 101, 1135, 1110, 2633, 1107, 1303,  119,  102],\n",
       "       [ 101, 1188, 1110, 2504,  119,  102,    0,    0],\n",
       "       [ 101, 1327,  170, 1632, 4250,  119,  102,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(nt, padding=True, truncation=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[ 101, 1135, 1110, 2633,  119,  102,    0],\n",
       "       [ 101, 1188, 1110, 2504,  119,  102,    0],\n",
       "       [ 101, 1327,  170, 1632, 4250,  119,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(nt, padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='tf',\n",
    "                max_length=4\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 101, 1135, 1110,  102],\n",
       "       [ 101, 1188, 1110,  102],\n",
       "       [ 101, 1327,  170,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT on a High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the basics of how GPT (Generative Pre-trained Transformer) models work. We'll do this in a step-by-step fashion, starting from the fundamental building blocks.\n",
    "\n",
    "### 1. Transformers\n",
    "\n",
    "At the heart of GPT is the Transformer architecture, which was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. The key innovation of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sequence relative to each other.\n",
    "\n",
    "### 2. Self-Attention Mechanism\n",
    "\n",
    "The main idea behind the self-attention mechanism is to compute a weighted sum of all words in a sequence based on their relevance to the current word. This is done by calculating three vectors for each word:\n",
    "- **Query (Q)**: Represents the current word.\n",
    "- **Key (K)**: Represents all the other words.\n",
    "- **Value (V)**: The actual values we want to sum up.\n",
    "\n",
    "The weight for each word is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q \\cdot K^T}{\\sqrt{d}} \\right) \\cdot V\n",
    "$$\n",
    "\n",
    "where \\(d\\) is the dimension of the query/key vectors.\n",
    "\n",
    "### 3. Multi-Head Attention\n",
    "\n",
    "In practice, the Transformer uses not just one set of Q, K, V matrices, but several. This allows the model to capture different types of relationships in the data. Each set of Q, K, V matrices is referred to as a \"head\", and the Transformer uses multiple heads in what's called \"multi-head attention\".\n",
    "\n",
    "### 4. Positional Encoding\n",
    "\n",
    "Since the Transformer does not have any inherent notion of the order of words, it uses positional encodings to give the model information about the position of words in a sequence. These encodings are added to the word embeddings at the input layer.\n",
    "\n",
    "### 5. GPT Specifics\n",
    "\n",
    "- **Masked Self-Attention**: While the original Transformer model is bidirectional (it looks at past and future words), GPT only looks at the past. This is done by masking future words in the self-attention mechanism.\n",
    "- **Stacked Layers**: Like other deep learning models, GPT stacks multiple layers of these Transformer blocks to capture more complex patterns.\n",
    "- **Fine-tuning for Tasks**: GPT can be fine-tuned for specific tasks by adding a simple output layer and training on task-specific data.\n",
    "\n",
    "### Simple Python Implementation\n",
    "\n",
    "Now, let's code a **very simplified** version of the self-attention mechanism to give you an idea of how it works in practice. Note that this will be a highly simplified example and won't capture the full complexity of GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def simple_self_attention(Q, K, V):\n",
    "    # Calculate the attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(Q.shape[1])\n",
    "    \n",
    "    # Apply the softmax to get the attention weights\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Multiply weights by the value matrix to get the final output\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define three simple word embeddings for \"Hello\", \"world\", and \"!\"\n",
    "hello_emb = np.array([1, 0, 0])\n",
    "world_emb = np.array([0, 1, 0])\n",
    "exclamation_emb = np.array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we'll use the same embeddings as Q, K, and V\n",
    "Q = np.array([hello_emb])\n",
    "K = np.array([hello_emb, world_emb, exclamation_emb])\n",
    "V = np.array([hello_emb, world_emb, exclamation_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47108308, 0.26445846, 0.26445846]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the self-attention output\n",
    "output = simple_self_attention(Q, K, V)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a weighted sum of the word embeddings based on their relevance to the word \"Hello\" in our simple setup. The output is a combination of the embeddings for \"Hello\", \"world\", and \"!\", with the largest weight on the \"Hello\" embedding (as expected, since the query was \"Hello\").\n",
    "\n",
    "This is a highly simplified demonstration, but it provides a basic idea of how self-attention computes a weighted sum of word embeddings based on their relevance to a given word.\n",
    "\n",
    "To fully understand GPT:\n",
    "\n",
    "1. Imagine stacking many such attention layers.\n",
    "2. Introduce the multi-head mechanism to capture various relationships simultaneously.\n",
    "3. Add positional encoding to embed sequence order information.\n",
    "4. Use a much larger vocabulary and embedding size.\n",
    "5. Consider the entire training process involving large datasets and powerful GPUs.\n",
    "\n",
    "However, with this basic understanding, you have a foundation for delving deeper into the intricacies of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Padding & Truncation\n",
    "\n",
    "Both \"padding\" and \"truncation\" are techniques used in the preprocessing of sequences (like sentences or paragraphs) before feeding them to models like transformers.\n",
    "\n",
    "### Padding:\n",
    "\n",
    "Imagine you have a tray of cupcake slots, but not all slots are filled with cupcakes. To make every slot look occupied, you might place empty cupcake liners in the empty slots. Similarly, in the world of sequence models:\n",
    "\n",
    "- **Padding** refers to adding \"dummy\" or \"filler\" values to a sequence to make it reach a certain desired length.\n",
    "\n",
    "For example, if our model expects sequences of length 5 and we have the sentence \"I love cats\", which has only 3 words, we might \"pad\" it with two extra \"dummy\" words to make its length 5: \"I love cats [PAD] [PAD]\".\n",
    "\n",
    "### Truncation:\n",
    "\n",
    "On the flip side, imagine you have too many cupcakes for the tray. You might have to remove a few to make them fit. Similarly, in sequence processing:\n",
    "\n",
    "- **Truncation** refers to shortening a sequence by removing some of its elements to ensure it doesn't exceed a certain length.\n",
    "\n",
    "For instance, if our model can handle sequences of a maximum length of 5 and we have the sentence \"I really really love cats\", we might \"truncate\" it to fit the model: \"I really really love\".\n",
    "\n",
    "### Why are these important?\n",
    "\n",
    "Transformers, and many other neural network architectures, often expect input sequences of a fixed size. However, in real-world data, sentences and paragraphs can vary widely in length. Padding and truncation help standardize the lengths of these sequences:\n",
    "\n",
    "1. **Padding** ensures shorter sequences can fit the expected input size.\n",
    "2. **Truncation** ensures longer sequences don't exceed the model's capacity.\n",
    "\n",
    "However, it's worth noting that truncation can lead to loss of information if not done carefully. Similarly, excessive padding can sometimes reduce the efficiency of the model's training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Bert Model\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is one of the most influential models in the field of Natural Language Processing (NLP). Let's unpack its name and relevance in simple terms:\n",
    "\n",
    "### BERT Explained:\n",
    "\n",
    "1. **Bidirectional**: Traditional language models used to read text either from left to right or right to left. But when we humans read to understand something, we consider the whole context, looking at words both before and after a given word. BERT does the same; it reads text both ways (bidirectionally) to understand the context better.\n",
    "\n",
    "2. **Encoder Representations**: BERT is designed to \"encode\" or convert text into a form (like vectors) that machines can understand. By \"representations,\" we mean the machine-friendly format that still captures the essence and meaning of the original text.\n",
    "\n",
    "3. **Transformers**: BERT is built on the transformer architecture, which we discussed earlier. It uses the self-attention mechanism to weigh the importance of different words in a sentence relative to a given word.\n",
    "\n",
    "### Relevance of BERT:\n",
    "\n",
    "1. **Pre-training on Large Data**: BERT is trained on massive amounts of text, like the entire Wikipedia. This gives it a broad understanding of language.\n",
    "\n",
    "2. **Fine-tuning on Specific Tasks**: Once pre-trained, BERT can be \"fine-tuned\" on a smaller, specific dataset for tasks like question-answering, sentiment analysis, or text classification.\n",
    "\n",
    "3. **State-of-the-Art Performance**: When introduced, BERT achieved state-of-the-art results on various NLP benchmarks, making it a go-to model for many NLP tasks.\n",
    "\n",
    "4. **Versatility**: BERT can be adapted to a wide range of NLP tasks with only minimal changes to the model. You essentially add a small layer on top for your specific task and fine-tune BERT on task-specific data.\n",
    "\n",
    "5. **Variants and Offshoots**: BERT's success led to the development of various versions and offshoots tailored for different needs, such as DistilBERT (a smaller, faster version) and RoBERTa (optimized with more data and training tweaks).\n",
    "\n",
    "In summary, BERT revolutionized NLP by providing a robust and versatile model that understands context better than previous models. Its introduction marked a significant shift in how researchers and practitioners approached NLP tasks, leading to the rapid development and adoption of transformer-based models in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://hilpisch.com/tpq_logo.png' width=\"35%\" align=\"right\">\n",
    "\n",
    "<br><br><a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">ai@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
